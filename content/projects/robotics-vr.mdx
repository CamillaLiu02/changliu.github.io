---
slug: robotics-vr
title: "(In Progress...)Immersive VR–Robotics Control System"
date: "2025-10-01"
tags: ["VR/XR", "Robotics", "Human-Robot Interaction", "ROS", "Research", "Motion Planning", "Linux", "TCP/IP"]
shortDescription: "Adapted and extended a VR–robotics framework for intuitive control of real-world robotic arms. Integrated Meta Quest 3, Unity, and ROS to enable immersive manipulation of a UR3e robotic arm through pose-based VR interaction."
role: "Undergraduate Researcher"
lab: "People and Robots Laboratory - University of Wisconsin-Madison"
tools: ["Meta Quest 3", "Unity (C#)", "ROS (Noetic)", "MoveIt", "Python", "UR3e Robotic Arm", "Inverse Kinematics", "Motion Planning"]
heroImage: "/images/projects/vr-robot/i2.png"
cardImage: "/images/projects/vr-robot/i.png"
galleryImages:
    - "/images/projects/vr-robot/i.png"
    - "/images/projects/vr-robot/i1.png"
    - "/images/projects/vr-robot/i2.png"
    - "/images/projects/vr-robot/i3.png"
    - "/images/projects/vr-robot/i4.png"
featured: true
---

## Overview

This research project investigates how immersive virtual reality interfaces can lower the barrier to robotic manipulation by enabling intuitive, pose-based control of real-world robotic arms. Rather than programming joint-level commands, users interact with a virtual representation of a robot in 3D space through a Meta Quest 3 headset, specifying target poses that are automatically translated into executable motion on a physical UR3e collaborative robotic arm.

The project builds upon an existing VR–robotics framework originally developed for a UR10 robot. My primary contribution focused on adapting, integrating, and validating this system for a different robot platform (UR3e), addressing hardware differences, kinematics constraints, and real-world deployment challenges.

### Research Context

This work is lead by **Callie Kim**, conducted within the [**People and Robots Laboratory**](https://peopleandrobots.wisc.edu/) at the University of Wisconsin-Madison, which focuses on human-robot interaction, collaborative manipulation, and intuitive interfaces for robotic control.

---

## Problem Statement & Motivation

Traditional robot programming requires technical expertise—users must understand joint angles, coordinate frames, and motion constraints. This creates a significant barrier for non-expert users who want to control robots for specific tasks.

**Key Research Questions:**
- Can immersive VR interfaces make robotic manipulation more intuitive?
- How do we bridge the gap between virtual interaction and physical execution?
- What are the usability and safety tradeoffs in immersive teleoperation?

By leveraging VR interaction patterns familiar from gaming and 3D modeling, this project explores whether users can effectively control complex robotic systems through spatial reasoning alone, without explicit programming knowledge.

---

## System Architecture

### Technology Stack

The system integrates three primary layers:

**1. VR Interface Layer (Meta Quest 3 + Unity)**
- Captures user hand poses and target specifications in 3D space
- Provides real-time visual feedback of the virtual robot state
- Communicates pose requests over network to ROS backend
![Unity](/images/projects/vr-robot/unity.png)

**2. ROS Control Pipeline (ROS Noetic + MoveIt)**
- Receives pose targets from Unity application
- Solves inverse kinematics using MoveIt's planning algorithms
- Plans collision-free trajectories
- Enforces safety constraints and workspace limits
- Sends execution commands to physical robot
![Unity](/images/projects/vr-robot/ros.jpg)

**3. Physical Robot Execution (UR3e Collaborative Arm)**
- Executes motion plans with real-time feedback
- Provides safety monitoring and force-torque feedback
- Reports execution status back through the pipeline
![Unity](/images/projects/vr-robot/ur3e.png)

### System Flow

```
VR User Input (Meta Quest 3)
        ↓
    Unity XR Interface
        ↓
    ROS Communication Bridge
        ↓
    MoveIt Motion Planning
    (IK Solving + Trajectory Planning)
        ↓
    UR3e Robot Controller
        ↓
    Physical Robot Execution
        ↓
    Feedback Loop → VR Visualization
```

---

## Key Technical Contributions

### 1. Hardware Adaptation: UR10 → UR3e

The original framework was designed for a UR10 robot. Adapting to UR3e required:

**Kinematic Reconfiguration**
- Modified URDF (robot description) files to reflect UR3e geometry
- Recalibrated inverse kinematics solvers for different arm reach and DOF
- Adjusted tool-center-point (TCP) offsets for end-effector configuration

**Workspace Adjustments**
- Redefined workspace boundaries (UR3e has ~500mm shorter reach than UR10)
- Updated collision models and safety constraints
- Validated joint limits and velocity constraints

**Coordinate Frame Alignment**
- Ensured consistent frame transformations between VR space and robot space
- Debugged frame misalignments that caused pose failures
- Implemented robust quaternion-to-Euler angle conversions

### 2. VR-to-Robot Pipeline Integration

**Pose-Based Control Architecture**
- Captured user hand poses from Meta Quest 3 controllers
- Transformed VR poses into robot base frame coordinates
- Implemented pose validation before kinematic solving
- Added fallback strategies for unreachable poses

**Real-Time Feedback Loop**
- Mirrored physical robot state back to VR visualization
- Displayed reachability status, collision warnings, and IK failures
- Provided haptic feedback through VR controllers for enhanced spatial awareness

### 3. Motion Planning & Constraint Handling

**MoveIt Integration**
- Configured MoveIt's planning scene with UR3e URDF and collision geometry
- Integrated RRTconnect motion planner for trajectory generation
- Implemented time-parameterization for smooth execution

**Safety & Validation**
- Self-collision checking to prevent impossible poses
- Environment collision checking for static obstacles
- Pose reachability validation before attempting IK solving
- Timeout handling for failed planning attempts

### 4. Debugging & Iterative Validation

**Key Challenges Encountered:**
- **Inverse Kinematics Failures** – Certain poses were mathematically unreachable or had multiple solutions; solved through pose validation and user feedback
- **Coordinate Frame Mismatches** – Virtual and physical spaces initially misaligned; debugged through frame visualization and careful transformation verification
- **Orientation Ambiguity** – Small changes in VR hand orientation produced large robot motions; addressed with orientation filtering and user guidance
- **Network Latency** – Delays between VR input and robot feedback; mitigated through predictive visualization and buffer strategies

**Testing Methodology**
- Iterative hardware-in-the-loop testing
- Systematic validation of workspace edges and constraint boundaries
- Usability pilot tests with early user interaction
- Safety validation through supervised testing with multiple pose types

---

## Current Work & Future Directions

### Adaptation
- Transitioning the VR–robotics control pipeline from UR10 to UR3e in progress
- Refining kinematics, motion planning, and pose alignment for reliable real-world execution
- Iteratively testing and debugging VR-to-robot consistency during integration

### User Research Study

To validate the system, I designed a comparative user study for future deployment.

#### Participants

- **n = 24** (12 per condition, counterbalanced)
- **Backgrounds**: 50% robotics students, 50% non-technical
- **VR experience**: 6 novices, 12 intermediate, 6 experts

#### Study Design

**Independent Variable**: Control method (VR vs. Keyboard/Mouse)

**Dependent Variables**:
- Task completion time
- Path efficiency (distance traveled)
- Collision rate
- NASA-TLX workload score
- User preference (Likert scale)

**Task**: Pick up a cube, navigate through a narrow gap, place it on a target platform

#### Procedure

1. 10-minute training with each interface
2. 5 task repetitions per interface (recorded)
3. Post-task questionnaire (SUS, NASA-TLX)
4. Semi-structured interview

**Current Status**:
- Recruiting volunteers to test VR interface effectiveness
- Measuring task completion time, accuracy, and cognitive load
- Collecting feedback on usability and spatial understanding
- Analyzing differences between expert and novice users

### Paper Publication
- Documenting framework adaptation process and technical insights
- Submitting to robotics/HRI conference next semester
- Sharing lessons learned in system integration and platform migration

### Future Extensions

- **Haptic Feedback Integration** – Adding force-feedback devices for proprioceptive guidance
- **Multi-Robot Coordination** – Extending framework to coordinate multiple robotic arms
- **Task Learning** – Enabling users to record and replay manipulation sequences
- **Eye-Gaze Integration** – Using gaze tracking for enhanced spatial targeting
- **Gesture-Based Commands** – Beyond pose specification, exploring natural gesture interfaces

---

## Learning Outcomes & Impact

This project provided hands-on experience with:

**Technical Skills**
- Adapted and extended research codebases to support different robotic hardware platforms
- Debugged multi-layer systems spanning VR interfaces, ROS middleware, and physical robotic hardware
- Analyzed inverse kinematics limitations, including singularities and workspace constraints
- Implemented real-time pose and command communication between Unity-based VR systems and ROS
- Developed and debugged software in Linux environments, including full OS installation, system configuration, package management, and development tooling setup
- Implemented TCP/IP-based communication for distributed systems and robotics pipelines across multiple machines
- Implemented and tested C/C++ components in system-level and performance-sensitive contexts
- Worked with multithreaded execution in Linux, including process coordination and debugging concurrency-related issues
- Built and deployed software on POSIX-compliant systems using shell scripting and command-line tools
- Worked with ROS-based systems utilizing real-time message passing, distributed nodes, and networked communication

**Research & Problem-Solving**
- Systematic debugging of hardware-software integration issues
- Validating technical solutions through iterative testing
- Documenting design decisions and alternative approaches
- Communicating technical complexity to diverse audiences

**Human-Centered Design**
- Evaluating usability tradeoffs in immersive interfaces
- Considering safety and user experience in system design
- Recognizing importance of feedback loops in human-robot interaction

### Broader Significance

This work contributes to a larger vision of **human-centered robotics**, where technical robustness, safety, and user experience must align for real-world deployment. By lowering the technical barrier to robot control, we can expand access to robotic capabilities beyond expert operators, enabling new applications in education, manufacturing, and personal robotics.

---

## Collaboration & Acknowledgments

The original VR–robotics framework and ROS pipeline were developed by collaborating researchers. My role focused on platform adaptation, system integration, and validation—ensuring that research infrastructure designed for one robot could be reliably extended to different hardware with appropriate technical modifications.

---

## Reflections

Building this system taught me that technical implementation is only one part of the story. Equally important is:

1. **Understanding System Boundaries** – Knowing why the original framework was built for UR10 helped me understand what would change with UR3e
2. **Embracing Iterative Validation** – Perfect solutions rarely work on first try; systematic testing revealed subtle coordinate-frame issues that would have been missed otherwise
3. **Balancing Robustness & Usability** – Safety constraints sometimes conflict with intuitive interaction; finding the right balance requires user feedback
4. **Documentation as Design** – Clear documentation of configuration choices enables future researchers to extend this work further

The intersection of VR interaction and robotic control remains a rich area for research, and I'm excited to see how user studies and continued refinement can make these interfaces even more effective.
